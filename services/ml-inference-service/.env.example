# Service Configuration
SERVICE_NAME=ml-inference-service
SERVICE_PORT=8005
ENV=development

# Triton Server
TRITON_URL=triton:8001
TRITON_HTTP_URL=triton:8000
TRITON_METRICS_URL=triton:8002

# Database
DB_HOST=timescaledb
DB_PORT=5432
DB_NAME=aipx_trading
DB_USER=aipx_user
DB_PASSWORD=your_db_password

# Redis
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0

# Model Configuration
MODEL_REPOSITORY=/models
MODEL_REFRESH_INTERVAL=300

# Feature Engineering
FEATURE_WINDOW_SIZE=60
FEATURE_NORMALIZATION=standard

# Inference Configuration
MAX_BATCH_SIZE=32
BATCH_TIMEOUT_MS=100
GPU_MEMORY_FRACTION=0.8

# Model Versions
LSTM_MODEL_VERSION=1
TRANSFORMER_MODEL_VERSION=1
ENSEMBLE_MODEL_VERSION=1

# Monitoring
METRICS_PORT=9090
LOG_LEVEL=INFO

# Training Configuration
TRAINING_DATA_PATH=/data/training
MODEL_CHECKPOINT_PATH=/models/checkpoints
TRAINING_EPOCHS=100
LEARNING_RATE=0.001
BATCH_SIZE=64

# S3 Configuration (for model storage)
AWS_REGION=ap-northeast-2
S3_BUCKET=aipx-ml-models
S3_PREFIX=models/

# Performance
WORKER_PROCESSES=4
WORKER_TIMEOUT=300
KEEP_ALIVE=5
