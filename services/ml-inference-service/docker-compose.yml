version: '3.8'

services:
  # NVIDIA Triton Inference Server
  triton:
    image: nvcr.io/nvidia/tritonserver:23.10-py3
    container_name: aipx-triton-server
    command: tritonserver --model-repository=/models --strict-model-config=false --log-verbose=1
    ports:
      - "8000:8000"  # HTTP
      - "8001:8001"  # gRPC
      - "8002:8002"  # Metrics
    volumes:
      - ./models:/models:ro
      - ./config/triton:/config:ro
    environment:
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    shm_size: '2gb'
    networks:
      - aipx-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ML Inference Service (FastAPI Gateway)
  ml-inference-service:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: aipx-ml-inference
    ports:
      - "8005:8005"
      - "9090:9090"  # Metrics
    volumes:
      - ./models:/models:ro
      - ./data:/data
      - ./src:/app/src:ro
    environment:
      - SERVICE_NAME=ml-inference-service
      - SERVICE_PORT=8005
      - ENV=production
      - TRITON_URL=triton:8001
      - TRITON_HTTP_URL=triton:8000
      - TRITON_METRICS_URL=triton:8002
      - DB_HOST=${DB_HOST:-timescaledb}
      - DB_PORT=${DB_PORT:-5432}
      - DB_NAME=${DB_NAME:-aipx_trading}
      - DB_USER=${DB_USER:-aipx_user}
      - DB_PASSWORD=${DB_PASSWORD}
      - REDIS_HOST=${REDIS_HOST:-redis}
      - REDIS_PORT=${REDIS_PORT:-6379}
      - LOG_LEVEL=INFO
    depends_on:
      triton:
        condition: service_healthy
    networks:
      - aipx-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Redis for caching
  redis:
    image: redis:7-alpine
    container_name: aipx-ml-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    networks:
      - aipx-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Prometheus for metrics
  prometheus:
    image: prom/prometheus:latest
    container_name: aipx-ml-prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    networks:
      - aipx-network
    restart: unless-stopped

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: aipx-ml-grafana
    ports:
      - "3001:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    depends_on:
      - prometheus
    networks:
      - aipx-network
    restart: unless-stopped

networks:
  aipx-network:
    external: true

volumes:
  redis-data:
  prometheus-data:
  grafana-data:
